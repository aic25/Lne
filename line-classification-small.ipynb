{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification - Small Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set GPU environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import GPUtil\n",
    "\n",
    "Availability=GPUtil.getAvailability(GPUtil.getGPUs())\n",
    "all_gpus = np.arange(3)\n",
    "available_gpu_indexes = [x for x in all_gpus if Availability[x]]\n",
    "NUMBER_OF_GPUS_TO_USE = len(available_gpu_indexes)\n",
    "# Set CUDA_DEVICE_ORDER so the IDs assigned by CUDA match those from nvidia-smi\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "# Set CUDA_VISIBLE_DEVICES to mask out all other GPUs than the first NUMBER_OF_GPUS_TO_USE available device id\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = ','.join(np.array(available_gpu_indexes[:NUMBER_OF_GPUS_TO_USE]).astype(str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set `keras` session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config=tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`imports`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings; warnings.filterwarnings('ignore') \n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import bz2, glob, os, xgboost, pickle\n",
    "\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import model_selection\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import decomposition, ensemble\n",
    "from collections import OrderedDict\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.pipeline import make_pipeline as make_pipeline_imb\n",
    "from imblearn.metrics import classification_report_imbalanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the `score` to evaluate each classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_classifier(y_true, y_pred):\n",
    "    return metrics.f1_score(\n",
    "        y_true, y_pred,\n",
    "        average='macro', #Calculate metrics for each label, and find their unweighted mean.\n",
    "        #This does not take label imbalance into account.\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data and create dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = bz2.BZ2File('training-data-small.txt.bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = bz2.BZ2File('test-data-small.txt.bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, texts = [],[]\n",
    "for i, line in enumerate(train_file.readlines(-1)):\n",
    "    content = line.decode(\"utf-8\").split(\"\\t\")\n",
    "    labels.append(content[0])\n",
    "    texts.append(content[1].split(\"\\n\")[0])\n",
    "    \n",
    "trainDF = pd.DataFrame()\n",
    "trainDF['text'] = texts\n",
    "trainDF['label'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "for i, line in enumerate(test_file.readlines(-1)):\n",
    "    content = line.decode(\"utf-8\").split(\"\\t\")\n",
    "    texts.append(content[0].split(\"\\n\")[0])\n",
    "    \n",
    "testDF = pd.DataFrame()\n",
    "testDF['text'] = texts\n",
    "testDF['label'] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split the dataset into training and validation datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(trainDF['text'], trainDF['label'], test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>X773579,Y2640,Y2072,Z4,Z15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>X166074297,X123474229,X147204623,X51578397,X23...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>X374616379,X773579,X344420902,Y1940,Y1705,Z4,Z...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>X103413307,X37875376,X62716332,X277692318,X344...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>X123474229,X551805107,X62716661,Y2307,Y2,Y1222...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label\n",
       "0                         X773579,Y2640,Y2072,Z4,Z15     0\n",
       "1  X166074297,X123474229,X147204623,X51578397,X23...     0\n",
       "2  X374616379,X773579,X344420902,Y1940,Y1705,Z4,Z...     1\n",
       "3  X103413307,X37875376,X62716332,X277692318,X344...     0\n",
       "4  X123474229,X551805107,X62716661,Y2307,Y2,Y1222...     0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I see that the text I need to process is a series of encoded words. I dont have access to the original words, therefore I cannot use words embeddings that are available online. The order of these encoded words may or maynot have a meaning. If there is a meaning I trust `CNN` or `RNN` based NN variants would be able to figure it out. \n",
    "\n",
    "Assuming the order of the encoded words does not have a meaning, I first try bag of words model. In this model, I segment each text into words (from the value counts it is safe to assume they are words), and count the number of times each word occurs in each text and assign each word an integer id. Each unique word will correspond to a feature.\n",
    "\n",
    "To find a classifier, I first obtain the classifier for various classification methods, optimize the parameters for it, and finally create a vote ensemble. \n",
    "\n",
    "I also tried various NN methods after that to see if there is any improvement in the performance. If I have enough time I want to create an ensemble of all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.717"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.7387777777777778"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'clf__alpha': 0.1, 'tfidf__use_idf': False, 'vect__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.751"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', MultinomialNB())])\n",
    "text_clf = text_clf.fit(train_x, train_y)\n",
    "\n",
    "# Performance of NB Classifier\n",
    "predicted = text_clf.predict(valid_x)\n",
    "metrics.accuracy_score(predicted, valid_y)\n",
    "\n",
    "# Grid Search\n",
    "# Creating a list of parameters for which I would like to do performance tuning. \n",
    "# Since my time is limited I cannot afford to explore the entire parameter space.\n",
    "parameters = {\n",
    "    'vect__ngram_range': [(1,1), (1,2), (1,3)], \n",
    "    'tfidf__use_idf': (True, False), \n",
    "    'clf__alpha': [.01,.1,1],\n",
    "}\n",
    "\n",
    "# Create an instance of the grid search by passing the classifier, parameters .\n",
    "gs_clf = GridSearchCV(text_clf, parameters, cv=5, n_jobs=-1)\n",
    "gs_clf = gs_clf.fit(train_x, train_y)\n",
    "\n",
    "gs_clf.best_score_\n",
    "gs_clf.best_params_\n",
    "\n",
    "predicted = gs_clf.predict(valid_x)\n",
    "metrics.accuracy_score(predicted, valid_y)\n",
    "\n",
    "models['nb'] = gs_clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.755"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.7692222222222223"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'clf-svm__alpha': 0.0001,\n",
       " 'clf-svm__learning_rate': 'constant',\n",
       " 'clf-svm__loss': 'hinge',\n",
       " 'tfidf__use_idf': False,\n",
       " 'vect__ngram_range': (1, 1)}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.763"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training Support Vector Machines - SVM and calculating its performance\n",
    "text_clf_svm = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
    "                         ('clf-svm', SGDClassifier(eta0=.1, penalty='l2', n_jobs=-1))]) \n",
    "\n",
    "text_clf_svm = text_clf_svm.fit(train_x, train_y)\n",
    "predicted_svm = text_clf_svm.predict(valid_x)\n",
    "metrics.accuracy_score(predicted_svm, valid_y)\n",
    "\n",
    "# Similarly doing grid search for SVM\n",
    "parameters_svm = {\n",
    "    'vect__ngram_range': [(1,1), (1,2), (1,3)], \n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'clf-svm__alpha': [.001,.0001],\n",
    "    'clf-svm__loss':('hinge', 'squared_hinge'),\n",
    "    'clf-svm__learning_rate':('constant','optimal','invscaling'),\n",
    "}\n",
    "\n",
    "gs_clf_svm = GridSearchCV(text_clf_svm, parameters_svm, cv=5, n_jobs=-1)\n",
    "gs_clf_svm = gs_clf_svm.fit(train_x, train_y)\n",
    "\n",
    "\n",
    "gs_clf_svm.best_score_\n",
    "gs_clf_svm.best_params_\n",
    "\n",
    "predicted_svm = gs_clf_svm.predict(valid_x)\n",
    "metrics.accuracy_score(predicted_svm, valid_y)\n",
    "\n",
    "models['svm'] = gs_clf_svm.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.755"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.7713333333333333"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'clf-svm__learning_rate': 'constant',\n",
       " 'clf-svm__loss': 'log',\n",
       " 'clf-svm__penalty': 'l2',\n",
       " 'tfidf__use_idf': False,\n",
       " 'vect__ngram_range': (1, 1)}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.769"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training Support Vector Machines - SVM and calculating its performance\n",
    "# Unfortunately 'hinge' loss cannot be used in the Vote Classifier I used in the ensemble.\n",
    "# I need to try another loss, although technically it may not be SVM anymore.\n",
    "text_clf_svm = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
    "                         ('clf-svm', SGDClassifier(eta0=.1, n_jobs=-1))]) \n",
    "\n",
    "text_clf_svm = text_clf_svm.fit(train_x, train_y)\n",
    "predicted_svm = text_clf_svm.predict(valid_x)\n",
    "metrics.accuracy_score(predicted_svm, valid_y)\n",
    "\n",
    "# Similarly doing grid search for SVM\n",
    "parameters_svm = {\n",
    "    'vect__ngram_range': [(1,1), (1,2), (1,3)], \n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'clf-svm__penalty':('l2','l1','elasticnet'),\n",
    "    'clf-svm__loss':('log', 'modified_huber', 'squared_hinge', 'perceptron', 'squared_loss', 'huber', 'epsilon_insensitive', 'squared_epsilon_insensitive'),\n",
    "    'clf-svm__learning_rate':('constant','optimal','invscaling'),\n",
    "}\n",
    "\n",
    "gs_clf_svm = GridSearchCV(text_clf_svm, parameters_svm, cv=5, n_jobs=-1)\n",
    "gs_clf_svm = gs_clf_svm.fit(train_x, train_y)\n",
    "\n",
    "\n",
    "gs_clf_svm.best_score_\n",
    "gs_clf_svm.best_params_\n",
    "\n",
    "predicted_svm = gs_clf_svm.predict(valid_x)\n",
    "metrics.accuracy_score(predicted_svm, valid_y)\n",
    "\n",
    "models['svm'] = gs_clf_svm.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.768"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.7713333333333333"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'clf-lr__C': 1,\n",
       " 'clf-lr__solver': 'newton-cg',\n",
       " 'tfidf__use_idf': True,\n",
       " 'vect__ngram_range': (1, 1)}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.768"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf_lr = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
    "                         ('clf-lr', linear_model.LogisticRegression(n_jobs=-1))])\n",
    "\n",
    "text_clf_lr = text_clf_lr.fit(train_x, train_y)\n",
    "predicted_lr = text_clf_lr.predict(valid_x)\n",
    "metrics.accuracy_score(predicted_lr, valid_y)\n",
    "\n",
    "parameters_lr = {'vect__ngram_range': [(1,1),(1,2)], \n",
    "                 'tfidf__use_idf': (True, False),\n",
    "                 'clf-lr__C': [.1,1,10],\n",
    "                 'clf-lr__solver':('newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'),\n",
    "                }\n",
    "\n",
    "gs_clf_lr = GridSearchCV(text_clf_lr, parameters_lr, cv=5, n_jobs=-1)\n",
    "gs_clf_lr = gs_clf_lr.fit(train_x, train_y)\n",
    "\n",
    "\n",
    "gs_clf_lr.best_score_\n",
    "gs_clf_lr.best_params_\n",
    "\n",
    "predicted_lr = gs_clf_lr.predict(valid_x)\n",
    "metrics.accuracy_score(predicted_lr, valid_y)\n",
    "\n",
    "models['lr'] = gs_clf_lr.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.723"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.7473333333333333"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'clf-rf__max_features': 'log2',\n",
       " 'clf-rf__n_estimators': 100,\n",
       " 'tfidf__use_idf': False,\n",
       " 'vect__ngram_range': (1, 1)}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.751"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf_rf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
    "                         ('clf-rf', ensemble.RandomForestClassifier(n_jobs=-1))])\n",
    "\n",
    "text_clf_rf = text_clf_rf.fit(train_x, train_y)\n",
    "predicted_rf = text_clf_rf.predict(valid_x)\n",
    "metrics.accuracy_score(predicted_rf, valid_y)\n",
    "\n",
    "parameters_rf = {'vect__ngram_range': [(1,1),(1,2)], \n",
    "                 'tfidf__use_idf': (True, False),\n",
    "                 'clf-rf__n_estimators': [10,50,100],\n",
    "                 'clf-rf__max_features': ('auto','sqrt','log2'),\n",
    "                }\n",
    "\n",
    "gs_clf_rf = GridSearchCV(text_clf_rf, parameters_rf, cv=5, n_jobs=-1)\n",
    "gs_clf_rf = gs_clf_rf.fit(train_x, train_y)\n",
    "\n",
    "gs_clf_rf.best_score_\n",
    "gs_clf_rf.best_params_\n",
    "\n",
    "predicted_rf = gs_clf_rf.predict(valid_x)\n",
    "metrics.accuracy_score(predicted_rf, valid_y)\n",
    "\n",
    "models['rf'] = gs_clf_rf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.757"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.7702222222222223"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'clf-xgb__max_depth': 3,\n",
       " 'clf-xgb__n_estimators': 200,\n",
       " 'tfidf__use_idf': False,\n",
       " 'vect__ngram_range': (1, 1)}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.76"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf_xgb = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
    "                         ('clf-xgb', xgboost.XGBClassifier(learning_rate=.1, n_jobs=-1))])\n",
    "\n",
    "text_clf_xgb = text_clf_xgb.fit(train_x, train_y)\n",
    "predicted_xgb = text_clf_xgb.predict(valid_x)\n",
    "metrics.accuracy_score(predicted_xgb, valid_y)\n",
    "\n",
    "parameters_xgb = {'vect__ngram_range': [(1,1),(1,2)], \n",
    "                  'tfidf__use_idf': (True, False),\n",
    "                  'clf-xgb__max_depth': [3,5],\n",
    "                  'clf-xgb__n_estimators':[100,200],\n",
    "                 }\n",
    "\n",
    "gs_clf_xgb = GridSearchCV(text_clf_xgb, parameters_xgb, cv=5, n_jobs=-1)\n",
    "gs_clf_xgb = gs_clf_xgb.fit(train_x, train_y)\n",
    "\n",
    "gs_clf_xgb.best_score_\n",
    "gs_clf_xgb.best_params_\n",
    "\n",
    "predicted_xgb = gs_clf_xgb.predict(valid_x)\n",
    "metrics.accuracy_score(predicted_xgb, valid_y)\n",
    "\n",
    "models['xgb'] = gs_clf_xgb.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.705"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.7082222222222222"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'tfidf__use_idf': False, 'vect__ngram_range': (1, 1)}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.705"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf_knn = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
    "                         ('clf-knn', KNeighborsClassifier(n_neighbors=10, p=1, algorithm='ball_tree', n_jobs=-1))])\n",
    "\n",
    "text_clf_knn = text_clf_knn.fit(train_x, train_y)\n",
    "predicted_knn = text_clf_knn.predict(valid_x)\n",
    "metrics.accuracy_score(predicted_knn, valid_y)\n",
    "\n",
    "parameters_knn = {\n",
    "    'vect__ngram_range': [(1,1), (1,2)], \n",
    "    'tfidf__use_idf': (True, False),\n",
    "#    'clf-knn__n_neighbors': [1,5,10],\n",
    "#    'clf-knn__weights':('distance','uniform'),\n",
    "#    'clf-knn__algorithm':('ball_tree', 'kd_tree', 'brute'),\n",
    "#    'clf-knn__p':[1,2],\n",
    "    }\n",
    "\n",
    "gs_clf_knn = GridSearchCV(text_clf_knn, parameters_knn, cv=5, n_jobs=-1)\n",
    "gs_clf_knn = gs_clf_knn.fit(train_x, train_y)\n",
    "\n",
    "gs_clf_knn.best_score_\n",
    "gs_clf_knn.best_params_\n",
    "\n",
    "predicted_knn = gs_clf_knn.predict(valid_x)\n",
    "metrics.accuracy_score(predicted_knn, valid_y)\n",
    "\n",
    "models['knn'] = gs_clf_knn.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.773"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.7696666666666667"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.763"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf_mlp = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
    "                         ('clf-mlp', MLPClassifier(hidden_layer_sizes=(512,32,64), early_stopping=True))])\n",
    "\n",
    "text_clf_mlp = text_clf_mlp.fit(train_x, train_y)\n",
    "predicted_mlp = text_clf_mlp.predict(valid_x)\n",
    "metrics.accuracy_score(predicted_mlp, valid_y)\n",
    "\n",
    "parameters_mlp = {\n",
    "#    'vect__ngram_range': [(1,1), (1,2)], \n",
    "#    'tfidf__use_idf': (True, False),\n",
    "#    'clf-mlp__activation':('logistic', 'tanh', 'relu'),\n",
    "#    'clf-mlp__solver':('lbfgs', 'sgd', 'adam'),\n",
    "#    'clf-mlp__learning_rate':('constant', 'invscaling', 'adaptive'),    \n",
    "    }\n",
    "# It was taking too long so I skip parameter optimization for MLP.\n",
    "\n",
    "gs_clf_mlp = GridSearchCV(text_clf_mlp, parameters_mlp,  cv=5)\n",
    "gs_clf_mlp = gs_clf_mlp.fit(train_x, train_y)\n",
    "\n",
    "gs_clf_mlp.best_score_\n",
    "gs_clf_mlp.best_params_\n",
    "\n",
    "predicted_mlp = gs_clf_mlp.predict(valid_x)\n",
    "metrics.accuracy_score(predicted_mlp, valid_y)\n",
    "\n",
    "models['mlp'] = gs_clf_mlp.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score each classifier, I have calculated so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('nb', 0.7215932914046123),\n",
       "             ('svm', 0.7424631449152079),\n",
       "             ('lr', 0.7396476265290091),\n",
       "             ('rf', 0.7049675642051009),\n",
       "             ('xgb', 0.7329998798499459),\n",
       "             ('knn', 0.6585011790391497),\n",
       "             ('mlp', 0.7191859948458189)])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_dict = OrderedDict([( key, score_classifier( y_true=valid_y, y_pred=model.predict(valid_x) ) )  for (key, model) in models.items()])\n",
    "score_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create voting ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('nb', Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 2), pre...ue, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False))]))],\n",
       "         flatten_transform=None, n_jobs=1, voting='hard', weights=None)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vc = VotingClassifier(estimators=list(models.items()))\n",
    "vc.fit(train_x, train_y)\n",
    "preds = vc.predict(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "models['vc'] = vc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate scores of the ensemble and see the improvement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8114828843995511"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_classifier(y_true=valid_y, y_pred=preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      0.90      0.87       627\n",
      "          1       0.80      0.71      0.76       373\n",
      "\n",
      "avg / total       0.83      0.83      0.83      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_true=valid_y, y_pred=preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.828\n"
     ]
    }
   ],
   "source": [
    "print(metrics.accuracy_score(y_true=valid_y, y_pred=preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "filename = 'vc_model_small.sav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(vc, open(filename, 'wb')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "# load the model from disk\n",
    "filename = 'vc_model_small.sav'\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "result = loaded_model.score(valid_x, valid_y)\n",
    "preds = loaded_model.predict(valid_x)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since test data is too large to predict all at once, divide it into small parts and combine them after prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict=[]\n",
    "locs = np.linspace(0,len(testDF),100,dtype=int)\n",
    "for ind, loc in enumerate(locs[:-1]):\n",
    "    test_predict.append(vc.predict(testDF.text.iloc[locs[ind]:locs[ind+1]]))\n",
    "test_predict = np.concatenate(test_predict[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDF.label = test_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDF.to_pickle('test-data-small-predictions.bz2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, try NN models that rely on word embeddings to see if they can do better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the language of the data is unknown, I create word embeddings from the data itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import text, sequence\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from gensim.sklearn_api import w2vmodel\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen= 150\n",
    "size =100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I should create the word embeddings from tha largest corpus I can get my hands on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = Word2Vec(testDF.text.apply(lambda x:x.split(\",\")), size=size, negative=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.wv.save_word2vec_format('model-small.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = KeyedVectors.load_word2vec_format('model-small.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(w2v.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = text.Tokenizer(lower=False)\n",
    "# token.fit_on_texts(train_x)\n",
    "token.fit_on_texts(trainDF.text)\n",
    "\n",
    "train_x_t2s = (token.texts_to_sequences(train_x)) \n",
    "valid_x_t2s = (token.texts_to_sequences(valid_x)) \n",
    "\n",
    "train_seq_x = sequence.pad_sequences(train_x_t2s, maxlen=maxlen)\n",
    "valid_seq_x = sequence.pad_sequences(valid_x_t2s, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create token-embedding mapping\n",
    "embedding_matrix = np.zeros((len(token.word_index) + 1, size))\n",
    "for word, i in token.word_index.items():\n",
    "    try:\n",
    "        embedding_vector = w2v[word]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    except:\n",
    "        embedding_matrix[i] =np.random.randn(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, models, optimizers\n",
    "from keras.backend import expand_dims\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb = EarlyStopping(monitor='val_acc', min_delta=0, patience=5, verbose=0, mode='auto', baseline=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"dropout_14/cond/Merge:0\", shape=(?, 150, 300), dtype=float32)\n",
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/50\n",
      "9000/9000 [==============================] - 21s 2ms/step - loss: 0.5489 - acc: 0.7281 - val_loss: 0.4669 - val_acc: 0.7890\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.78900, saving model to best_models/cnn_lstm/weights.hdf5\n",
      "Epoch 2/50\n",
      "9000/9000 [==============================] - 21s 2ms/step - loss: 0.4942 - acc: 0.7641 - val_loss: 0.4639 - val_acc: 0.7830\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.78900\n",
      "Epoch 3/50\n",
      "9000/9000 [==============================] - 21s 2ms/step - loss: 0.4924 - acc: 0.7618 - val_loss: 0.4659 - val_acc: 0.7810\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.78900\n",
      "Epoch 4/50\n",
      "9000/9000 [==============================] - 21s 2ms/step - loss: 0.4862 - acc: 0.7709 - val_loss: 0.4689 - val_acc: 0.7830\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.78900\n",
      "Epoch 5/50\n",
      "9000/9000 [==============================] - 21s 2ms/step - loss: 0.4836 - acc: 0.7656 - val_loss: 0.4625 - val_acc: 0.7810\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.78900\n",
      "Epoch 6/50\n",
      "9000/9000 [==============================] - 21s 2ms/step - loss: 0.4833 - acc: 0.7676 - val_loss: 0.4826 - val_acc: 0.7800\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.78900\n",
      "CNN, Word Embeddings 0.789\n"
     ]
    }
   ],
   "source": [
    "filepath=\"best_models/cnn_lstm/weights.hdf5\"\n",
    "chkpt = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "# BLSTM-2DCNN\n",
    "def create_cnn_lstm():\n",
    "    inp = layers.Input(shape=(maxlen, ))\n",
    "    x = layers.Embedding(max_features+1, size, weights=[embedding_matrix], input_length=maxlen)(inp)\n",
    "    x = layers.Dropout(0.5)(x)    \n",
    "\n",
    "    x = layers.Bidirectional(layers.CuDNNLSTM(300, return_sequences=True), merge_mode='sum')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "#     x = expand_dims(x, axis=-1)\n",
    "    print(x)\n",
    "    x = layers.Reshape((150, 300, 1))(x)\n",
    "\n",
    "    x = layers.Conv2D(100, kernel_size=(5,5), padding='valid', kernel_initializer='glorot_uniform')(x)\n",
    "    x = layers.MaxPool2D(pool_size=(5,5))(x)\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    x = layers.Dense(1, activation = \"sigmoid\",  kernel_regularizer=regularizers.l2(.00001))(x)\n",
    "    \n",
    "    parallel_model = models.Model(inputs = inp, outputs = x)\n",
    "    parallel_model = multi_gpu_model(parallel_model, gpus=NUMBER_OF_GPUS_TO_USE)\n",
    "    parallel_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return parallel_model\n",
    "\n",
    "clf_cnn_lstm = create_cnn_lstm()\n",
    "\n",
    "hist_cnn_lstm = clf_cnn_lstm.fit(train_seq_x, train_y, epochs=50, callbacks=[cb, chkpt], validation_data=(valid_seq_x, valid_y))\n",
    "clf_cnn_lstm.load_weights(filepath)\n",
    "predicted_cnn_lstm = clf_cnn_lstm.predict(valid_seq_x)\n",
    "print(\"CNN, Word Embeddings\",  metrics.accuracy_score(np.round(predicted_cnn_lstm).astype(int).astype(str), valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_models['cnn_lstm'] = clf_cnn_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/50\n",
      "9000/9000 [==============================] - 3s 334us/step - loss: 0.6271 - acc: 0.6436 - val_loss: 0.5916 - val_acc: 0.6800\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.68000, saving model to best_models/cnn/weights.hdf5\n",
      "Epoch 2/50\n",
      "9000/9000 [==============================] - 2s 211us/step - loss: 0.5621 - acc: 0.7139 - val_loss: 0.5432 - val_acc: 0.7380\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.68000 to 0.73800, saving model to best_models/cnn/weights.hdf5\n",
      "Epoch 3/50\n",
      "9000/9000 [==============================] - 2s 210us/step - loss: 0.5261 - acc: 0.7417 - val_loss: 0.5120 - val_acc: 0.7470\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.73800 to 0.74700, saving model to best_models/cnn/weights.hdf5\n",
      "Epoch 4/50\n",
      "9000/9000 [==============================] - 2s 209us/step - loss: 0.5009 - acc: 0.7561 - val_loss: 0.4967 - val_acc: 0.7690\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.74700 to 0.76900, saving model to best_models/cnn/weights.hdf5\n",
      "Epoch 5/50\n",
      "9000/9000 [==============================] - 2s 211us/step - loss: 0.4774 - acc: 0.7713 - val_loss: 0.4912 - val_acc: 0.7630\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.76900\n",
      "Epoch 6/50\n",
      "9000/9000 [==============================] - 2s 206us/step - loss: 0.4657 - acc: 0.7803 - val_loss: 0.4850 - val_acc: 0.7710\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.76900 to 0.77100, saving model to best_models/cnn/weights.hdf5\n",
      "Epoch 7/50\n",
      "9000/9000 [==============================] - 2s 208us/step - loss: 0.4465 - acc: 0.7904 - val_loss: 0.4869 - val_acc: 0.7620\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.77100\n",
      "Epoch 8/50\n",
      "9000/9000 [==============================] - 2s 212us/step - loss: 0.4328 - acc: 0.8017 - val_loss: 0.4803 - val_acc: 0.7690\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.77100\n",
      "Epoch 9/50\n",
      "9000/9000 [==============================] - 2s 215us/step - loss: 0.4156 - acc: 0.8088 - val_loss: 0.4867 - val_acc: 0.7650\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.77100\n",
      "Epoch 10/50\n",
      "9000/9000 [==============================] - 2s 208us/step - loss: 0.3974 - acc: 0.8212 - val_loss: 0.4908 - val_acc: 0.7640\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.77100\n",
      "Epoch 11/50\n",
      "9000/9000 [==============================] - 2s 193us/step - loss: 0.3785 - acc: 0.8340 - val_loss: 0.5354 - val_acc: 0.7410\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.77100\n",
      "CNN, Word Embeddings 0.771\n"
     ]
    }
   ],
   "source": [
    "filepath=\"best_models/cnn/weights.hdf5\"\n",
    "chkpt = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "def create_cnn():\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((maxlen, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(token.word_index)+1, size, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    #embedding_layer = w2v.get_keras_embedding()(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.2)(embedding_layer)\n",
    "\n",
    "    # Add the convolutional Layer\n",
    "    conv_layer = layers.Convolution1D(100, 10, activation=\"relu\")(embedding_layer)\n",
    "\n",
    "    # Add the pooling Layer\n",
    "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\n",
    "    output_layer1 = layers.Dropout(0.2)(output_layer1)\n",
    "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)    \n",
    "    model = multi_gpu_model(model, gpus=NUMBER_OF_GPUS_TO_USE)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "clf_cnn = create_cnn()\n",
    "\n",
    "hist_cnn = clf_cnn.fit(train_seq_x, train_y, epochs=50, callbacks=[cb, chkpt], validation_data=(valid_seq_x, valid_y))\n",
    "clf_cnn.load_weights(filepath)\n",
    "predicted_cnn = clf_cnn.predict(valid_seq_x)\n",
    "print(\"CNN, Word Embeddings\",  metrics.accuracy_score(np.round(predicted_cnn).astype(int).astype(str), valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_models['cnn'] = clf_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/50\n",
      "9000/9000 [==============================] - 6s 703us/step - loss: 0.6528 - acc: 0.6421 - val_loss: 0.6505 - val_acc: 0.6270\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.62700, saving model to best_models/rnn_lstm/weights.hdf5\n",
      "Epoch 2/50\n",
      "9000/9000 [==============================] - 5s 526us/step - loss: 0.6360 - acc: 0.6431 - val_loss: 0.6307 - val_acc: 0.6260\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.62700\n",
      "Epoch 3/50\n",
      "9000/9000 [==============================] - 5s 527us/step - loss: 0.6261 - acc: 0.6556 - val_loss: 0.6107 - val_acc: 0.6510\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.62700 to 0.65100, saving model to best_models/rnn_lstm/weights.hdf5\n",
      "Epoch 4/50\n",
      "9000/9000 [==============================] - 5s 531us/step - loss: 0.6038 - acc: 0.6706 - val_loss: 0.5926 - val_acc: 0.7070\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.65100 to 0.70700, saving model to best_models/rnn_lstm/weights.hdf5\n",
      "Epoch 5/50\n",
      "9000/9000 [==============================] - 5s 536us/step - loss: 0.5911 - acc: 0.6859 - val_loss: 0.5815 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.70700\n",
      "Epoch 6/50\n",
      "9000/9000 [==============================] - 5s 539us/step - loss: 0.5724 - acc: 0.6994 - val_loss: 0.5829 - val_acc: 0.6660\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.70700\n",
      "Epoch 7/50\n",
      "9000/9000 [==============================] - 5s 536us/step - loss: 0.5711 - acc: 0.7062 - val_loss: 0.5712 - val_acc: 0.6870\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.70700\n",
      "Epoch 8/50\n",
      "9000/9000 [==============================] - 5s 537us/step - loss: 0.5514 - acc: 0.7202 - val_loss: 0.5393 - val_acc: 0.7400\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.70700 to 0.74000, saving model to best_models/rnn_lstm/weights.hdf5\n",
      "Epoch 9/50\n",
      "9000/9000 [==============================] - 5s 537us/step - loss: 0.5435 - acc: 0.7258 - val_loss: 0.5361 - val_acc: 0.7440\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.74000 to 0.74400, saving model to best_models/rnn_lstm/weights.hdf5\n",
      "Epoch 10/50\n",
      "9000/9000 [==============================] - 5s 543us/step - loss: 0.5399 - acc: 0.7296 - val_loss: 0.5173 - val_acc: 0.7620\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.74400 to 0.76200, saving model to best_models/rnn_lstm/weights.hdf5\n",
      "Epoch 11/50\n",
      "9000/9000 [==============================] - 5s 543us/step - loss: 0.5379 - acc: 0.7288 - val_loss: 0.5114 - val_acc: 0.7520\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.76200\n",
      "Epoch 12/50\n",
      "9000/9000 [==============================] - 5s 540us/step - loss: 0.5408 - acc: 0.7239 - val_loss: 0.6160 - val_acc: 0.6610\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.76200\n",
      "Epoch 13/50\n",
      "9000/9000 [==============================] - 5s 539us/step - loss: 0.5965 - acc: 0.6899 - val_loss: 0.5673 - val_acc: 0.7130\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.76200\n",
      "Epoch 14/50\n",
      "9000/9000 [==============================] - 5s 540us/step - loss: 0.5663 - acc: 0.7094 - val_loss: 0.5375 - val_acc: 0.7530\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.76200\n",
      "Epoch 15/50\n",
      "9000/9000 [==============================] - 5s 541us/step - loss: 0.5510 - acc: 0.7207 - val_loss: 0.5297 - val_acc: 0.7560\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.76200\n",
      "RNN-LSTM, Word Embeddings 0.762\n"
     ]
    }
   ],
   "source": [
    "filepath=\"best_models/rnn_lstm/weights.hdf5\"\n",
    "chkpt = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "def create_rnn_lstm():\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((maxlen, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(token.word_index) + 1, size, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.2)(embedding_layer)\n",
    "\n",
    "    # Add the LSTM Layer\n",
    "    lstm_layer = layers.CuDNNLSTM(100)(embedding_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
    "    output_layer1 = layers.Dropout(0.2)(output_layer1)\n",
    "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model = multi_gpu_model(model, gpus=NUMBER_OF_GPUS_TO_USE)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "clf_rnn = create_rnn_lstm()\n",
    "hist_rnn = clf_rnn.fit(train_seq_x, train_y, epochs=50, callbacks=[cb, chkpt], validation_data=(valid_seq_x, valid_y))\n",
    "clf_rnn.load_weights(filepath)\n",
    "predicted_rnn = clf_rnn.predict(valid_seq_x)\n",
    "print(\"RNN-LSTM, Word Embeddings\",  metrics.accuracy_score(np.round(predicted_rnn).astype(int).astype(str), valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_models['rnn_lstm'] = clf_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/50\n",
      "9000/9000 [==============================] - 6s 721us/step - loss: 0.6529 - acc: 0.6417 - val_loss: 0.6537 - val_acc: 0.6270\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.62700, saving model to best_models/rnn_gru/weights.hdf5\n",
      "Epoch 2/50\n",
      "9000/9000 [==============================] - 5s 537us/step - loss: 0.6286 - acc: 0.6591 - val_loss: 0.5970 - val_acc: 0.6890\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.62700 to 0.68900, saving model to best_models/rnn_gru/weights.hdf5\n",
      "Epoch 3/50\n",
      "9000/9000 [==============================] - 5s 537us/step - loss: 0.5941 - acc: 0.6834 - val_loss: 0.5759 - val_acc: 0.7180\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.68900 to 0.71800, saving model to best_models/rnn_gru/weights.hdf5\n",
      "Epoch 4/50\n",
      "9000/9000 [==============================] - 5s 538us/step - loss: 0.5835 - acc: 0.6977 - val_loss: 0.5639 - val_acc: 0.7220\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.71800 to 0.72200, saving model to best_models/rnn_gru/weights.hdf5\n",
      "Epoch 5/50\n",
      "9000/9000 [==============================] - 5s 536us/step - loss: 0.5641 - acc: 0.7071 - val_loss: 0.5681 - val_acc: 0.6940\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.72200\n",
      "Epoch 6/50\n",
      "9000/9000 [==============================] - 5s 536us/step - loss: 0.5708 - acc: 0.7118 - val_loss: 0.5692 - val_acc: 0.6940\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.72200\n",
      "Epoch 7/50\n",
      "9000/9000 [==============================] - 5s 542us/step - loss: 0.5507 - acc: 0.7192 - val_loss: 0.5282 - val_acc: 0.7520\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.72200 to 0.75200, saving model to best_models/rnn_gru/weights.hdf5\n",
      "Epoch 8/50\n",
      "9000/9000 [==============================] - 5s 539us/step - loss: 0.5413 - acc: 0.7280 - val_loss: 0.5066 - val_acc: 0.7580\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.75200 to 0.75800, saving model to best_models/rnn_gru/weights.hdf5\n",
      "Epoch 9/50\n",
      "9000/9000 [==============================] - 5s 539us/step - loss: 0.5341 - acc: 0.7298 - val_loss: 0.5084 - val_acc: 0.7670\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.75800 to 0.76700, saving model to best_models/rnn_gru/weights.hdf5\n",
      "Epoch 10/50\n",
      "9000/9000 [==============================] - 5s 539us/step - loss: 0.5341 - acc: 0.7290 - val_loss: 0.5203 - val_acc: 0.7450\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.76700\n",
      "Epoch 11/50\n",
      "9000/9000 [==============================] - 5s 539us/step - loss: 0.5308 - acc: 0.7362 - val_loss: 0.5032 - val_acc: 0.7820\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.76700 to 0.78200, saving model to best_models/rnn_gru/weights.hdf5\n",
      "Epoch 12/50\n",
      "9000/9000 [==============================] - 5s 539us/step - loss: 0.5335 - acc: 0.7311 - val_loss: 0.5827 - val_acc: 0.7250\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.78200\n",
      "Epoch 13/50\n",
      "9000/9000 [==============================] - 5s 549us/step - loss: 0.5381 - acc: 0.7331 - val_loss: 0.4992 - val_acc: 0.7790\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.78200\n",
      "Epoch 14/50\n",
      "9000/9000 [==============================] - 5s 538us/step - loss: 0.5287 - acc: 0.7399 - val_loss: 0.4947 - val_acc: 0.7620\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.78200\n",
      "Epoch 15/50\n",
      "9000/9000 [==============================] - 5s 539us/step - loss: 0.5205 - acc: 0.7407 - val_loss: 0.4874 - val_acc: 0.7770\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.78200\n",
      "Epoch 16/50\n",
      "9000/9000 [==============================] - 5s 539us/step - loss: 0.5594 - acc: 0.7124 - val_loss: 0.5095 - val_acc: 0.7630\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.78200\n",
      "RNN-GRU, Word Embeddings 0.782\n"
     ]
    }
   ],
   "source": [
    "filepath=\"best_models/rnn_gru/weights.hdf5\"\n",
    "chkpt = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "def create_rnn_gru():\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((maxlen, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(token.word_index) + 1, size, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.2)(embedding_layer)\n",
    "\n",
    "    # Add the GRU Layer\n",
    "    lstm_layer = layers.CuDNNGRU(100)(embedding_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
    "    output_layer1 = layers.Dropout(0.2)(output_layer1)\n",
    "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model = multi_gpu_model(model, gpus=NUMBER_OF_GPUS_TO_USE)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "clf_gru = create_rnn_gru()\n",
    "hist_gru = clf_gru.fit(train_seq_x, train_y, epochs=50, callbacks=[cb, chkpt], validation_data=(valid_seq_x, valid_y))\n",
    "clf_gru.load_weights(filepath)\n",
    "predicted_gru = clf_gru.predict(valid_seq_x)\n",
    "print(\"RNN-GRU, Word Embeddings\",  metrics.accuracy_score(np.round(predicted_gru).astype(int).astype(str), valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_models['rnn_gru'] = clf_gru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/50\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.6534 - acc: 0.6420 - val_loss: 0.6545 - val_acc: 0.6270\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.62700, saving model to best_models/bidirectional_rnn/weights.hdf5\n",
      "Epoch 2/50\n",
      "9000/9000 [==============================] - 9s 977us/step - loss: 0.6349 - acc: 0.6472 - val_loss: 0.6561 - val_acc: 0.6360\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.62700 to 0.63600, saving model to best_models/bidirectional_rnn/weights.hdf5\n",
      "Epoch 3/50\n",
      "9000/9000 [==============================] - 9s 973us/step - loss: 0.6112 - acc: 0.6701 - val_loss: 0.5867 - val_acc: 0.7020\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.63600 to 0.70200, saving model to best_models/bidirectional_rnn/weights.hdf5\n",
      "Epoch 4/50\n",
      "9000/9000 [==============================] - 9s 989us/step - loss: 0.5959 - acc: 0.6850 - val_loss: 0.5830 - val_acc: 0.6980\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.70200\n",
      "Epoch 5/50\n",
      "9000/9000 [==============================] - 9s 993us/step - loss: 0.5826 - acc: 0.6914 - val_loss: 0.5550 - val_acc: 0.7340\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.70200 to 0.73400, saving model to best_models/bidirectional_rnn/weights.hdf5\n",
      "Epoch 6/50\n",
      "9000/9000 [==============================] - 9s 989us/step - loss: 0.5676 - acc: 0.7071 - val_loss: 0.5414 - val_acc: 0.7440\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.73400 to 0.74400, saving model to best_models/bidirectional_rnn/weights.hdf5\n",
      "Epoch 7/50\n",
      "9000/9000 [==============================] - 9s 989us/step - loss: 0.6342 - acc: 0.6532 - val_loss: 0.5796 - val_acc: 0.7240\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.74400\n",
      "Epoch 8/50\n",
      "9000/9000 [==============================] - 9s 993us/step - loss: 0.6360 - acc: 0.6930 - val_loss: 0.6683 - val_acc: 0.6270\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.74400\n",
      "Epoch 9/50\n",
      "9000/9000 [==============================] - 9s 988us/step - loss: 0.6447 - acc: 0.6426 - val_loss: 0.6233 - val_acc: 0.6270\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.74400\n",
      "Epoch 10/50\n",
      "9000/9000 [==============================] - 9s 986us/step - loss: 0.5910 - acc: 0.6778 - val_loss: 0.5617 - val_acc: 0.7190\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.74400\n",
      "Epoch 11/50\n",
      "9000/9000 [==============================] - 9s 990us/step - loss: 0.5694 - acc: 0.7080 - val_loss: 0.5656 - val_acc: 0.6950\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.74400\n",
      "RNN-Gbidirectional, Word Embeddings 0.744\n"
     ]
    }
   ],
   "source": [
    "filepath=\"best_models/bidirectional_rnn/weights.hdf5\"\n",
    "chkpt = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "def create_bidirectional_rnn():\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((maxlen, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(token.word_index) + 1, size, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.2)(embedding_layer)\n",
    "\n",
    "    # Add the LSTM Layer\n",
    "    lstm_layer = layers.Bidirectional(layers.CuDNNGRU(100))(embedding_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
    "    output_layer1 = layers.Dropout(0.2)(output_layer1)\n",
    "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model = multi_gpu_model(model, gpus=NUMBER_OF_GPUS_TO_USE)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "clf_rnnbi = create_bidirectional_rnn()\n",
    "hist_rnnbi = clf_rnnbi.fit(train_seq_x, train_y, epochs=50, callbacks=[cb, chkpt], validation_data=(valid_seq_x, valid_y))\n",
    "clf_rnnbi.load_weights(filepath)\n",
    "predicted_rnnbi = clf_rnnbi.predict(valid_seq_x)\n",
    "print(\"RNN-Gbidirectional, Word Embeddings\",  metrics.accuracy_score(np.round(predicted_rnnbi).astype(int).astype(str), valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_models['rnnbi'] = clf_rnnbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/50\n",
      "9000/9000 [==============================] - 4s 390us/step - loss: 0.5906 - acc: 0.6817 - val_loss: 0.5407 - val_acc: 0.7330\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.73300, saving model to best_models/rcnn/weights.hdf5\n",
      "Epoch 2/50\n",
      "9000/9000 [==============================] - 2s 208us/step - loss: 0.5157 - acc: 0.7433 - val_loss: 0.5051 - val_acc: 0.7520\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.73300 to 0.75200, saving model to best_models/rcnn/weights.hdf5\n",
      "Epoch 3/50\n",
      "9000/9000 [==============================] - 2s 208us/step - loss: 0.4743 - acc: 0.7728 - val_loss: 0.4801 - val_acc: 0.7770\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.75200 to 0.77700, saving model to best_models/rcnn/weights.hdf5\n",
      "Epoch 4/50\n",
      "9000/9000 [==============================] - 2s 211us/step - loss: 0.4469 - acc: 0.7931 - val_loss: 0.4750 - val_acc: 0.7860\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.77700 to 0.78600, saving model to best_models/rcnn/weights.hdf5\n",
      "Epoch 5/50\n",
      "9000/9000 [==============================] - 2s 208us/step - loss: 0.4233 - acc: 0.8060 - val_loss: 0.5001 - val_acc: 0.7660\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.78600\n",
      "Epoch 6/50\n",
      "9000/9000 [==============================] - 2s 208us/step - loss: 0.3926 - acc: 0.8239 - val_loss: 0.5019 - val_acc: 0.7620\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.78600\n",
      "Epoch 7/50\n",
      "9000/9000 [==============================] - 2s 209us/step - loss: 0.3666 - acc: 0.8370 - val_loss: 0.5174 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.78600\n",
      "Epoch 8/50\n",
      "9000/9000 [==============================] - 2s 210us/step - loss: 0.3350 - acc: 0.8529 - val_loss: 0.5179 - val_acc: 0.7710\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.78600\n",
      "Epoch 9/50\n",
      "9000/9000 [==============================] - 2s 210us/step - loss: 0.2921 - acc: 0.8771 - val_loss: 0.5651 - val_acc: 0.7490\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.78600\n",
      "RCNN, Word Embeddings 0.786\n"
     ]
    }
   ],
   "source": [
    "filepath=\"best_models/rcnn/weights.hdf5\"\n",
    "chkpt = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "def create_rcnn():\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((maxlen, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(token.word_index) + 1, size, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.2)(embedding_layer)\n",
    "    \n",
    "    # Add the recurrent layer\n",
    "    rnn_layer = layers.Bidirectional(layers.CuDNNGRU(50, return_sequences=True))(embedding_layer)\n",
    "    \n",
    "    # Add the convolutional Layer\n",
    "    conv_layer = layers.Convolution1D(100, 10, activation=\"selu\")(embedding_layer)\n",
    "\n",
    "    # Add the pooling Layer\n",
    "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"selu\")(pooling_layer)\n",
    "    output_layer1 = layers.Dropout(0.2)(output_layer1)\n",
    "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model = multi_gpu_model(model, gpus=NUMBER_OF_GPUS_TO_USE)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "clf_rcnn = create_rcnn()\n",
    "hist_rcnn = clf_rcnn.fit(train_seq_x, train_y, epochs=50, callbacks=[cb, chkpt], validation_data=(valid_seq_x, valid_y))\n",
    "clf_rcnn.load_weights(filepath)\n",
    "predicted_rcnn = clf_rcnn.predict(valid_seq_x)\n",
    "print(\"RCNN, Word Embeddings\",  metrics.accuracy_score(np.round(predicted_rcnn).astype(int).astype(str), valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_models['rcnn'] = clf_rcnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('cnn', 0.7553395278373403),\n",
       "             ('rnn_lstm', 0.7388048235505863),\n",
       "             ('rnnbi', 0.723870132671772),\n",
       "             ('rcnn', 0.7694402785678578),\n",
       "             ('cnn_lstm', 0.7711891333273331),\n",
       "             ('rnn_gru', 0.7594785318039994)])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_dict = OrderedDict([( key, score_classifier( y_true=valid_y, y_pred=np.round(model.predict(valid_seq_x)).astype(int).astype(str) ) )  for (key, model) in nn_models.items()])\n",
    "score_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_test = text.Tokenizer(lower=False)\n",
    "# token.fit_on_texts(train_x)\n",
    "token_test.fit_on_texts(testDF.text)\n",
    "\n",
    "test_x_seq = (token_test.texts_to_sequences(testDF.text)) \n",
    "\n",
    "test_x_seq_pad = sequence.pad_sequences(test_x_seq, maxlen=maxlen)\n",
    "\n",
    "testDF.label = np.round(nn_models['cnn_lstm'].predict(test_x_seq_pad)).astype(int).astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the model that performed best to the voting classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      0.85      0.84       627\n",
      "          1       0.73      0.68      0.71       373\n",
      "\n",
      "avg / total       0.79      0.79      0.79      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_true=valid_y, y_pred=np.round(nn_models['cnn_lstm'].predict(valid_seq_x)).astype(int).astype(str)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "# load the model from disk\n",
    "filename = 'vc_model_small.sav'\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "loaded_model = loaded_model.fit(train_x, train_y)\n",
    "result = loaded_model.score(valid_x, valid_y)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      0.90      0.87       627\n",
      "          1       0.80      0.71      0.76       373\n",
      "\n",
      "avg / total       0.83      0.83      0.83      1000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_true=valid_y, y_pred=loaded_model.predict(valid_x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The voting model performed better and I dont have enough time to create an ensemble of all or tune NN models, I will be using the voting classifier to create predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
