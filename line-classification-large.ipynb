{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification for the Large Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the data is too large, I will be using the parameters I obtained when I optimized the models for the small dataset. It would take too much time to optimize for a dataset this large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly I start with setting environment variables and imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import GPUtil\n",
    "\n",
    "\n",
    "Availability=GPUtil.getAvailability(GPUtil.getGPUs())\n",
    "all_gpus = np.arange(3)\n",
    "available_gpu_indexes = [x for x in all_gpus if Availability[x]]\n",
    "# NUMBER_OF_GPUS_TO_USE = len(available_gpu_indexes)\n",
    "NUMBER_OF_GPUS_TO_USE = 1\n",
    "# Set CUDA_DEVICE_ORDER so the IDs assigned by CUDA match those from nvidia-smi\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "# Set CUDA_VISIBLE_DEVICES to mask out all other GPUs than the first NUMBER_OF_GPUS_TO_USE available device id\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = ','.join(np.array(available_gpu_indexes[:NUMBER_OF_GPUS_TO_USE]).astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config=tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 15965674896586113992\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 11209333146\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 8353315950670369382\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:05:00.0, compute capability: 5.2\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings; warnings.filterwarnings('ignore') \n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import bz2, glob, os, xgboost, pickle\n",
    "\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import model_selection\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import decomposition, ensemble\n",
    "from collections import OrderedDict\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.pipeline import make_pipeline as make_pipeline_imb\n",
    "from imblearn.metrics import classification_report_imbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_classifier(y_true, y_pred):\n",
    "    return metrics.f1_score(\n",
    "        y_true, y_pred,\n",
    "        average='macro', #Calculate metrics for each label, and find their unweighted mean.\n",
    "        #This does not take label imbalance into account.\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = bz2.BZ2File('training-data-large.txt.bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = bz2.BZ2File('test-data-large.txt.bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, texts = [],[]\n",
    "for i, line in enumerate(train_file.readlines(-1)):\n",
    "    content = line.decode(\"utf-8\").split(\"\\t\")\n",
    "    labels.append(content[0])\n",
    "    texts.append(content[1].split(\"\\n\")[0])\n",
    "    \n",
    "trainDF = pd.DataFrame()\n",
    "trainDF['text'] = texts\n",
    "trainDF['label'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "for i, line in enumerate(test_file.readlines(-1)):\n",
    "    content = line.decode(\"utf-8\").split(\"\\t\")\n",
    "    texts.append(content[0].split(\"\\n\")[0])\n",
    "    \n",
    "testDF = pd.DataFrame()\n",
    "testDF['text'] = texts\n",
    "testDF['label'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into training and validation datasets \n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(trainDF['text'], trainDF['label'], test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to time constraints, I use the parameters I obtained when I optimized for the small dataset and skip parameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 48s, sys: 5.05 s, total: 2min 53s\n",
      "Wall time: 2min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer(ngram_range=(1,2))), \n",
    "    ('tfidf', TfidfTransformer(use_idf=False)), \n",
    "    ('clf', MultinomialNB(alpha=.1))])\n",
    "text_clf = text_clf.fit(train_x, train_y)\n",
    "\n",
    "# Performance of NB Classifier\n",
    "predicted = text_clf.predict(valid_x)\n",
    "metrics.accuracy_score(predicted, valid_y)\n",
    "\n",
    "models['nb'] = text_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82925"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the voting classifier does not accept `hinge` loss therefore I am not going to use it in the ensemble. This calculation is to see how it perform compared to others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 6s, sys: 2.4 s, total: 1min 8s\n",
      "Wall time: 1min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Training Support Vector Machines - SVM and calculating its performance\n",
    "text_clf_svm_o = Pipeline([\n",
    "    ('vect', CountVectorizer(ngram_range=(1,1))), \n",
    "    ('tfidf', TfidfTransformer(use_idf=False)),\n",
    "    ('clf-svm', SGDClassifier(loss='hinge', learning_rate='constant', alpha=.0001, eta0=.1, penalty='l2', n_jobs=10))]) \n",
    "\n",
    "text_clf_svm_o = text_clf_svm_o.fit(train_x, train_y)\n",
    "predicted_svm = text_clf_svm_o.predict(valid_x)\n",
    "metrics.accuracy_score(predicted_svm, valid_y)\n",
    "\n",
    "\n",
    "# models['svm'] = gs_clf_svm.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78628"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(predicted_svm, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77962"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training Support Vector Machines - SVM and calculating its performance\n",
    "text_clf_svm = Pipeline([\n",
    "    ('vect', CountVectorizer(ngram_range=(1,1))), \n",
    "    ('tfidf', TfidfTransformer(use_idf=False)),\n",
    "    ('clf-svm', SGDClassifier(loss='log',penalty='l2',learning_rate='constant',eta0=.1,n_jobs=10))]) \n",
    "\n",
    "text_clf_svm = text_clf_svm.fit(train_x, train_y)\n",
    "predicted_svm = text_clf_svm.predict(valid_x)\n",
    "metrics.accuracy_score(predicted_svm, valid_y)\n",
    "\n",
    "models['svm'] = text_clf_svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The creation of `logistic regression` classifier could not complete due to memory error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf_lr = Pipeline([\n",
    "    ('vect', CountVectorizer(ngram_range=(1,1))), \n",
    "    ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "    ('clf-lr', linear_model.LogisticRegression(solver='newton-cg', C=1., n_jobs=10))])\n",
    "\n",
    "text_clf_lr = text_clf_lr.fit(train_x, train_y)\n",
    "predicted_lr = text_clf_lr.predict(valid_x)\n",
    "metrics.accuracy_score(predicted_lr, valid_y)\n",
    "\n",
    "models['lr'] = text_clf_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84519"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf_rf = Pipeline([\n",
    "    ('vect', CountVectorizer(ngram_range=(1,1))), \n",
    "    ('tfidf', TfidfTransformer(use_idf=False)),\n",
    "    ('clf-rf', ensemble.RandomForestClassifier(n_estimators =100, max_features='log2', n_jobs=10))])\n",
    "\n",
    "text_clf_rf = text_clf_rf.fit(train_x, train_y)\n",
    "predicted_rf = text_clf_rf.predict(valid_x)\n",
    "metrics.accuracy_score(predicted_rf, valid_y)\n",
    "\n",
    "models['rf'] = text_clf_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.79054"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf_xgb = Pipeline([\n",
    "    ('vect', CountVectorizer(ngram_range=(1,1))), \n",
    "    ('tfidf', TfidfTransformer(use_idf=False)),\n",
    "    ('clf-xgb', xgboost.XGBClassifier(n_estimators = 200, max_depth = 3, learning_rate=.1, n_jobs=10))])\n",
    "\n",
    "text_clf_xgb = text_clf_xgb.fit(train_x, train_y)\n",
    "predicted_xgb = text_clf_xgb.predict(valid_x)\n",
    "metrics.accuracy_score(predicted_xgb, valid_y)\n",
    "\n",
    "models['xgb'] = text_clf_xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The creation of `K-Nearest neighbors` classifier could not complete due to memory error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf_knn = Pipeline([\n",
    "    ('vect', CountVectorizer(ngram_range=(1,1))), \n",
    "    ('tfidf', TfidfTransformer(use_idf=False)),\n",
    "    ('clf-knn', KNeighborsClassifier(n_neighbors=5, leaf_size=10, p=1, algorithm='ball_tree', n_jobs=10))])\n",
    "\n",
    "text_clf_knn = text_clf_knn.fit(train_x, train_y)\n",
    "predicted_knn = text_clf_knn.predict(valid_x)\n",
    "metrics.accuracy_score(predicted_knn, valid_y)\n",
    "\n",
    "models['knn'] = text_clf_knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was taking too long, I skip `MLP` as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf_mlp = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf-mlp', MLPClassifier(hidden_layer_sizes=(512,256), batch_size=5000, early_stopping=True))])\n",
    "\n",
    "text_clf_mlp = text_clf_mlp.fit(train_x, train_y)\n",
    "predicted_mlp = text_clf_mlp.predict(valid_x)\n",
    "metrics.accuracy_score(predicted_mlp, valid_y)\n",
    "\n",
    "models['mlp'] = text_clf_mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see what we got so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('svm', 0.7617825515018082),\n",
       "             ('xgb', 0.7836097379768439),\n",
       "             ('rf', 0.8282508249869445),\n",
       "             ('nb', 0.8193059610665339)])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_dict = OrderedDict([( key, score_classifier( y_true=valid_y, y_pred=model.predict(valid_x) ) )  for (key, model) in models.items()])\n",
    "score_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('svm', Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), pr...e,\n",
       "         use_idf=False)), ('clf', MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True))]))],\n",
       "         flatten_transform=None, n_jobs=1, voting='hard', weights=None)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "vc = VotingClassifier(estimators=list(models.items()))\n",
    "vc.fit(train_x, train_y)\n",
    "preds = vc.predict(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "models['vc'] = vc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ensemble offered increased performance as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8405271434195396"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_classifier(y_true=valid_y, y_pred=preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      0.90      0.87       627\n",
      "          1       0.80      0.71      0.76       373\n",
      "\n",
      "avg / total       0.83      0.83      0.83      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_true=valid_y, y_pred=preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.828\n"
     ]
    }
   ],
   "source": [
    "print(metrics.accuracy_score(y_true=valid_y, y_pred=preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "filename = 'vc_model-large.sav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(vc, open(filename, 'wb')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "# load the model from disk\n",
    "filename = 'vc_model.sav'\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "result = loaded_model.score(valid_x, valid_y)\n",
    "preds = loaded_model.predict(valid_x)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time I was able to predict all at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "testDF.label = vc.predict(testDF.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDF.to_pickle('test-data-large-predictions.bz2', compression='bz2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I skip trying and tuning other NN models and go with the voting classifier due to time constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
